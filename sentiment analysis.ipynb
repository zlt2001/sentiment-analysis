{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f988ef96",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6c060",
   "metadata": {},
   "source": [
    "æ˜¯çš„ï¼Œ**BERT çš„åˆ†è¯å™¨ï¼ˆå¦‚ `BertTokenizer`ï¼‰ä¼šå°†æ•´ä¸ªå¥å­å½“ä½œä¸€ä¸ªæ•´ä½“æ¥è¿›è¡Œåˆ†è¯ï¼Œå¹¶è¿”å›**ï¼š\n",
    "\n",
    "* æ•´ä½“çš„ token åˆ—è¡¨ï¼›\n",
    "* æ¯ä¸ª token çš„ IDï¼›\n",
    "* æ•´ä½“çš„ token æ•°ï¼ˆè¿™æ‰æ˜¯æ‰€è°“çš„â€œå¥å­é•¿åº¦â€ï¼‰ï¼›\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ä¸¾ä¸ªä¾‹å­ï¼ˆè‹±æ–‡ï¼‰\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"The stock performance was extremely disappointing.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token count:\", len(tokens))\n",
    "```\n",
    "\n",
    "è¾“å‡ºå¯èƒ½ä¸ºï¼š\n",
    "\n",
    "```text\n",
    "Tokens: ['the', 'stock', 'performance', 'was', 'extremely', 'dis', '##appoint', '##ing', '.']\n",
    "Token count: 9\n",
    "```\n",
    "\n",
    "â†’ æ³¨æ„ï¼š`disappointing` è¢«æ‹†æˆäº† `['dis', '##appoint', '##ing']`ï¼Œä½†æ•´ä½“ä½œä¸ºä¸€ä¸ªå¥å­è¢«å¤„ç†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ä¸­æ–‡ä¹Ÿä¸€æ ·\n",
    "\n",
    "```python\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "text = \"æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒï¼Œä½†æ˜¯æ˜å¤©æœ‰ç‚¹æ‹…å¿§ã€‚\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print(\"Token count:\", len(tokens))\n",
    "```\n",
    "\n",
    "è¾“å‡ºç¤ºä¾‹ï¼š\n",
    "\n",
    "```text\n",
    "['æˆ‘', 'ä»Šå¤©', 'å¾ˆ', 'å¼€', 'å¿ƒ', 'ï¼Œ', 'ä½†æ˜¯', 'æ˜å¤©', 'æœ‰', 'ç‚¹', 'æ‹…', 'å¿§', 'ã€‚']\n",
    "Token count: 13\n",
    "```\n",
    "\n",
    "â†’ æ•´å¥è¯æ˜¯ä½œä¸º**ä¸€ä¸ªè¾“å…¥æ•´ä½“**é€å…¥ BERT çš„ï¼Œåˆ†è¯åè‡ªç„¶å¾—åˆ°æ€» token æ•°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "> âœ”ï¸ **BERT åˆ†è¯å™¨ä¸æ˜¯é€è¯åˆ†ï¼Œè€Œæ˜¯â€œæ•´å¥åˆ†â€**ï¼Œå®ƒä¼šæ•´ä½“è€ƒè™‘å¥å­çš„ç»“æ„ã€æ ‡ç‚¹ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼Œç„¶åæ‹†æˆä¸€ç»„ tokenï¼Œå¹¶è¾“å‡º token æ•°ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ åœ¨ç”¨ï¼š\n",
    "\n",
    "```python\n",
    "tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "```\n",
    "\n",
    "æ—¶ï¼ŒBERT ä¼šï¼š\n",
    "\n",
    "* **è‡ªåŠ¨è®¡ç®—æ•´ä¸ªè¾“å…¥çš„ token é•¿åº¦**ï¼›\n",
    "* è¶…å‡º `max_length` å°± **æˆªæ–­**ï¼›\n",
    "* ä¸è¶³å°± **å¡«å……ï¼ˆpadï¼‰**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "å¦‚æœä½ æƒ³æˆ‘å¸®ä½ å†™ä¸ªå°è„šæœ¬ï¼š**åˆ†æä¸€æ‰¹æ–‡æœ¬çš„ token å¹³å‡é•¿åº¦ã€ä¸­ä½æ•°ã€æœ€å¤§å€¼**ï¼Œæ¥å†³å®šæ˜¯å¦ç”¨ `max_len=128` æ˜¯ä¸æ˜¯åˆé€‚ï¼Œæˆ‘å¯ä»¥é©¬ä¸Šå†™ã€‚éœ€è¦å—ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class VADataset(Dataset):\n",
    "    # max_len=128æ˜¯ç±»ä¼¼å¥å­æœ€é•¿é•¿åº¦ï¼Œä¸€ä¸ªtokenæ˜¯ä¸€ä¸ªå¥å­\n",
    "    def __init__(self, dataframe, tokenizer_name='bert-base-chinese', max_len=128,column_name = 'text'):\n",
    "        # TODO\n",
    "        # è¿˜æœ‰æŒ‰ç…§phraseã€wordæ¥å‘½åçš„\n",
    "        self.texts = dataframe[column_name].tolist()\n",
    "        self.labels = dataframe[['Valence_Mean', 'Arousal_Mean']].values.astype(float)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fd912",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0baefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class VAModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-chinese'):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        return self.regressor(cls_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd7c9b",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    pearson_v = pearsonr(preds[:, 0], labels[:, 0])[0]\n",
    "    pearson_a = pearsonr(preds[:, 1], labels[:, 1])[0]\n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'Pearson_V': pearson_v,\n",
    "        'Pearson_A': pearson_a\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220dd415",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# å®šä¹‰æ ¹ç›®å½•\n",
    "root_dir = 'ChineseEmoBank'\n",
    "\n",
    "# åŒ¹é…æ‰€æœ‰ *_all.csv æˆ– *_all_SD.csv æ–‡ä»¶\n",
    "all_csv_paths = glob.glob(os.path.join(root_dir, '*', '*all*.csv'))\n",
    "\n",
    "# è¯»å–å¹¶åˆå¹¶\n",
    "df_list = []\n",
    "for file in all_csv_paths:\n",
    "    # df = pd.read_csv(file)\n",
    "    pd.read_csv(file, encoding='utf-8', nrows=10)\n",
    "    df['source_file'] = os.path.basename(file)  # å¯é€‰ï¼šè®°å½•æ¥æº\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0160bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆå¹¶å®Œæˆï¼Œæ€»æ ·æœ¬æ•°ï¼š 2949\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "No.\tText\tValence_Mean\tArousal_Mean\tValence_SD\tArousal_SD\tCategory",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "894d635e-d8df-452e-ad58-d93a9a4dade4",
       "rows": [
        [
         "0",
         "2525\täººç”Ÿæ²’æ³•å‡è¨­ï¼Œæˆ‘æ²’æ³•å‡è¨­å¦‚æœæˆ‘åçš„åš´é‡å¿ƒæƒ…æœƒæ€æ¨£ï¼Œæˆ‘åªçŸ¥é“å¦‚ä½•åœ¨è‡ªå·±ç¾æœ‰çš„ç‹€æ³ä¸‹ï¼Œå„˜å¯èƒ½ä¿æŒæ¨‚è§€æ…‹åº¦ï¼Œé€™æ˜¯æˆ‘å¸Œæœ›åœ¨é€™è£¡åˆ†äº«çš„ã€‚\t6.571\t3.714\t0.495\t1.03\tnews"
        ],
        [
         "1",
         "1045\tã€Œå¥½èŠå¡¢å ±å°ã€æŒ‡å‡ºï¼Œä»Šå¤©å…¬ä½ˆçš„å€«æ•¦å½±è©•äººå”æœƒçï¼Œè‡ºç£å°æ¼”æå®‰æ“Šæ•—åŒæ¨£å…¥åœå¥§æ–¯å¡é‡‘åƒçæœ€ä½³å°æ¼”çš„å¥§åœ°åˆ©å°æ¼”éº¥å¯æ¼¢å…§å…‹ï¼Œè´å¾—å¹´åº¦æœ€ä½³å°æ¼”çã€‚\t6.714\t6.167\t0.7\t0.687\tnews"
        ],
        [
         "2",
         "138\tæƒ³åˆ°æ•…å®®çš„çƒé´‰éƒ½æœ‰äº†æ­¡å–œçš„æ„Ÿè¦º.\t6.250\t4.333\t0.968\t0.943\tbook"
        ],
        [
         "3",
         "1781\tæ”¿é»¨å°±æ˜¯ç†å¿µçš„çµåˆï¼Œè¯äººä¸–ç•Œæ—¢ç„¶åªå‰©ä¸‹é€™å…©å€‹æ”¿é»¨é‚„ç›¸ä¿¡ç†æ™ºåŠç§‘å­¸ï¼Œèˆ‡æˆ‘ç†å¿µç›¸ç¬¦ï¼Œé‚£éº¼æˆ‘åªå¥½æ”¯æŒä»–å€‘äº†ã€‚\t6.125\t4.333\t0.599\t0.943\tpolitical"
        ],
        [
         "4",
         "2695\t107å¹´åº¦ç¶œåˆæ‰€å¾—ç¨…ç”³å ±31æ—¥æœ€å¾Œ1å¤©ï¼Œæ“šçµ±è¨ˆï¼Œåˆ©ç”¨é›»å­ç”³å ±çš„æ°‘çœ¾å·²è¶…é6æˆï¼Œåˆ°ç¨…å‹™å–®ä½å…ƒç¾å ´åˆ©ç”¨äººå·¥ç”³å ±çš„æ°‘çœ¾æ˜é¡¯æ¸›å°‘ï¼Œä½œæ¥­éç¨‹ä¹Ÿé †æš¢è¨±å¤šã€‚\t6.375\t4.000\t0.484\t0.577\tnews"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.\\tText\\tValence_Mean\\tArousal_Mean\\tValence_SD\\tArousal_SD\\tCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2525\\täººç”Ÿæ²’æ³•å‡è¨­ï¼Œæˆ‘æ²’æ³•å‡è¨­å¦‚æœæˆ‘åçš„åš´é‡å¿ƒæƒ…æœƒæ€æ¨£ï¼Œæˆ‘åªçŸ¥é“å¦‚ä½•åœ¨è‡ªå·±ç¾æœ‰çš„ç‹€æ³ä¸‹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1045\\tã€Œå¥½èŠå¡¢å ±å°ã€æŒ‡å‡ºï¼Œä»Šå¤©å…¬ä½ˆçš„å€«æ•¦å½±è©•äººå”æœƒçï¼Œè‡ºç£å°æ¼”æå®‰æ“Šæ•—åŒæ¨£å…¥åœå¥§æ–¯å¡é‡‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138\\tæƒ³åˆ°æ•…å®®çš„çƒé´‰éƒ½æœ‰äº†æ­¡å–œçš„æ„Ÿè¦º.\\t6.250\\t4.333\\t0.968\\t0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1781\\tæ”¿é»¨å°±æ˜¯ç†å¿µçš„çµåˆï¼Œè¯äººä¸–ç•Œæ—¢ç„¶åªå‰©ä¸‹é€™å…©å€‹æ”¿é»¨é‚„ç›¸ä¿¡ç†æ™ºåŠç§‘å­¸ï¼Œèˆ‡æˆ‘ç†å¿µç›¸ç¬¦ï¼Œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2695\\t107å¹´åº¦ç¶œåˆæ‰€å¾—ç¨…ç”³å ±31æ—¥æœ€å¾Œ1å¤©ï¼Œæ“šçµ±è¨ˆï¼Œåˆ©ç”¨é›»å­ç”³å ±çš„æ°‘çœ¾å·²è¶…é6æˆï¼Œåˆ°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No.\\tText\\tValence_Mean\\tArousal_Mean\\tValence_SD\\tArousal_SD\\tCategory\n",
       "0  2525\\täººç”Ÿæ²’æ³•å‡è¨­ï¼Œæˆ‘æ²’æ³•å‡è¨­å¦‚æœæˆ‘åçš„åš´é‡å¿ƒæƒ…æœƒæ€æ¨£ï¼Œæˆ‘åªçŸ¥é“å¦‚ä½•åœ¨è‡ªå·±ç¾æœ‰çš„ç‹€æ³ä¸‹...                     \n",
       "1  1045\\tã€Œå¥½èŠå¡¢å ±å°ã€æŒ‡å‡ºï¼Œä»Šå¤©å…¬ä½ˆçš„å€«æ•¦å½±è©•äººå”æœƒçï¼Œè‡ºç£å°æ¼”æå®‰æ“Šæ•—åŒæ¨£å…¥åœå¥§æ–¯å¡é‡‘...                     \n",
       "2  138\\tæƒ³åˆ°æ•…å®®çš„çƒé´‰éƒ½æœ‰äº†æ­¡å–œçš„æ„Ÿè¦º.\\t6.250\\t4.333\\t0.968\\t0....                     \n",
       "3  1781\\tæ”¿é»¨å°±æ˜¯ç†å¿µçš„çµåˆï¼Œè¯äººä¸–ç•Œæ—¢ç„¶åªå‰©ä¸‹é€™å…©å€‹æ”¿é»¨é‚„ç›¸ä¿¡ç†æ™ºåŠç§‘å­¸ï¼Œèˆ‡æˆ‘ç†å¿µç›¸ç¬¦ï¼Œ...                     \n",
       "4  2695\\t107å¹´åº¦ç¶œåˆæ‰€å¾—ç¨…ç”³å ±31æ—¥æœ€å¾Œ1å¤©ï¼Œæ“šçµ±è¨ˆï¼Œåˆ©ç”¨é›»å­ç”³å ±çš„æ°‘çœ¾å·²è¶…é6æˆï¼Œåˆ°...                     "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# åŒ¹é…æ–‡ä»¶è·¯å¾„ï¼ˆç¡®ä¿åœ¨å½“å‰å·¥ä½œç›®å½•æˆ–ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰\n",
    "csv_paths = sorted(glob.glob('ChineseEmoBank/CVAT_SD/CVAT_*_SD.csv'))\n",
    "\n",
    "# é€ä¸ªè¯»å–å¹¶åˆå¹¶\n",
    "df_list = [pd.read_csv(path, encoding='utf-8', on_bad_lines='skip') for path in csv_paths]\n",
    "df_cvat_merged = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# è¾“å‡ºä¿¡æ¯\n",
    "print(\"âœ… åˆå¹¶å®Œæˆï¼Œæ€»æ ·æœ¬æ•°ï¼š\", len(df_cvat_merged))\n",
    "df_cvat_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b404bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜åˆå¹¶åçš„ DataFrame åˆ°æœ¬åœ° CSV æ–‡ä»¶\n",
    "df_cvat_merged.to_csv('ChineseEmoBank/CVAT_SD/CVAT_all_SD.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9ff825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# æ•°æ®åŠ è½½\n",
    "df_phrase = pd.read_csv('ChineseEmoBank/CVAP_SD/CVAP_all_SD.csv', sep='\\t')  # åŒ…å« text, valence, arousal åˆ—\n",
    "df_sentence = pd.read_csv('ChineseEmoBank/CVAS_SD/CVAS_all.csv', sep='\\t')  # åŒ…å« text, valence, arousal åˆ—\n",
    "df_text = pd.read_csv('ChineseEmoBank/CVAT_SD/CVAT_all_SD.csv', sep='\\t')  # åŒ…å« text, valence, arousal åˆ—\n",
    "df_word = pd.read_csv('ChineseEmoBank/CVAW_SD/CVAW_all_SD.csv', sep='\\t')  # åŒ…å« text, valence, arousal åˆ—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "from model import VAModel\n",
    "from dataset import VADataset\n",
    "from utils import compute_metrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df = df.sample(frac=0.8)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_set = VADataset(train_df)\n",
    "val_set = VADataset(val_df)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "\n",
    "# æ¨¡å‹åˆå§‹åŒ–\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAModel().to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=len(train_loader) * 5)\n",
    "\n",
    "best_mae = float('inf')\n",
    "patience, patience_counter = 3, 0\n",
    "\n",
    "# è®­ç»ƒ\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # éªŒè¯\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(**inputs)\n",
    "            preds.append(output)\n",
    "            targets.append(labels)\n",
    "    \n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    metrics = compute_metrics(preds, targets)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val MAE: {metrics['MAE']:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if metrics['MAE'] < best_mae:\n",
    "        best_mae = metrics['MAE']\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
