{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f988ef96",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6c060",
   "metadata": {},
   "source": [
    "是的，**BERT 的分词器（如 `BertTokenizer`）会将整个句子当作一个整体来进行分词，并返回**：\n",
    "\n",
    "* 整体的 token 列表；\n",
    "* 每个 token 的 ID；\n",
    "* 整体的 token 数（这才是所谓的“句子长度”）；\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 举个例子（英文）\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"The stock performance was extremely disappointing.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token count:\", len(tokens))\n",
    "```\n",
    "\n",
    "输出可能为：\n",
    "\n",
    "```text\n",
    "Tokens: ['the', 'stock', 'performance', 'was', 'extremely', 'dis', '##appoint', '##ing', '.']\n",
    "Token count: 9\n",
    "```\n",
    "\n",
    "→ 注意：`disappointing` 被拆成了 `['dis', '##appoint', '##ing']`，但整体作为一个句子被处理。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 中文也一样\n",
    "\n",
    "```python\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "text = \"我今天很开心，但是明天有点担忧。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print(\"Token count:\", len(tokens))\n",
    "```\n",
    "\n",
    "输出示例：\n",
    "\n",
    "```text\n",
    "['我', '今天', '很', '开', '心', '，', '但是', '明天', '有', '点', '担', '忧', '。']\n",
    "Token count: 13\n",
    "```\n",
    "\n",
    "→ 整句话是作为**一个输入整体**送入 BERT 的，分词后自然得到总 token 数。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 总结一句话：\n",
    "\n",
    "> ✔️ **BERT 分词器不是逐词分，而是“整句分”**，它会整体考虑句子的结构、标点、特殊字符等，然后拆成一组 token，并输出 token 数。\n",
    "\n",
    "这就是为什么你在用：\n",
    "\n",
    "```python\n",
    "tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "```\n",
    "\n",
    "时，BERT 会：\n",
    "\n",
    "* **自动计算整个输入的 token 长度**；\n",
    "* 超出 `max_length` 就 **截断**；\n",
    "* 不足就 **填充（pad）**。\n",
    "\n",
    "---\n",
    "\n",
    "如果你想我帮你写个小脚本：**分析一批文本的 token 平均长度、中位数、最大值**，来决定是否用 `max_len=128` 是不是合适，我可以马上写。需要吗？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class VADataset(Dataset):\n",
    "    # max_len=128是类似句子最长长度，一个token是一个句子\n",
    "    def __init__(self, dataframe, tokenizer_name='bert-base-chinese', max_len=128,column_name = 'text'):\n",
    "        # TODO\n",
    "        # 还有按照phrase、word来命名的\n",
    "        self.texts = dataframe[column_name].tolist()\n",
    "        self.labels = dataframe[['Valence_Mean', 'Arousal_Mean']].values.astype(float)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fd912",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0baefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class VAModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-chinese'):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        return self.regressor(cls_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd7c9b",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    pearson_v = pearsonr(preds[:, 0], labels[:, 0])[0]\n",
    "    pearson_a = pearsonr(preds[:, 1], labels[:, 1])[0]\n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'Pearson_V': pearson_v,\n",
    "        'Pearson_A': pearson_a\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220dd415",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# 定义根目录\n",
    "root_dir = 'ChineseEmoBank'\n",
    "\n",
    "# 匹配所有 *_all.csv 或 *_all_SD.csv 文件\n",
    "all_csv_paths = glob.glob(os.path.join(root_dir, '*', '*all*.csv'))\n",
    "\n",
    "# 读取并合并\n",
    "df_list = []\n",
    "for file in all_csv_paths:\n",
    "    # df = pd.read_csv(file)\n",
    "    pd.read_csv(file, encoding='utf-8', nrows=10)\n",
    "    df['source_file'] = os.path.basename(file)  # 可选：记录来源\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0160bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 合并完成，总样本数： 2949\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "No.\tText\tValence_Mean\tArousal_Mean\tValence_SD\tArousal_SD\tCategory",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "894d635e-d8df-452e-ad58-d93a9a4dade4",
       "rows": [
        [
         "0",
         "2525\t人生沒法假設，我沒法假設如果我吐的嚴重心情會怎樣，我只知道如何在自己現有的狀況下，儘可能保持樂觀態度，這是我希望在這裡分享的。\t6.571\t3.714\t0.495\t1.03\tnews"
        ],
        [
         "1",
         "1045\t「好萊塢報導」指出，今天公佈的倫敦影評人協會獎，臺灣導演李安擊敗同樣入圍奧斯卡金像獎最佳導演的奧地利導演麥可漢內克，贏得年度最佳導演獎。\t6.714\t6.167\t0.7\t0.687\tnews"
        ],
        [
         "2",
         "138\t想到故宮的烏鴉都有了歡喜的感覺.\t6.250\t4.333\t0.968\t0.943\tbook"
        ],
        [
         "3",
         "1781\t政黨就是理念的結合，華人世界既然只剩下這兩個政黨還相信理智及科學，與我理念相符，那麼我只好支持他們了。\t6.125\t4.333\t0.599\t0.943\tpolitical"
        ],
        [
         "4",
         "2695\t107年度綜合所得稅申報31日最後1天，據統計，利用電子申報的民眾已超過6成，到稅務單位元現場利用人工申報的民眾明顯減少，作業過程也順暢許多。\t6.375\t4.000\t0.484\t0.577\tnews"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.\\tText\\tValence_Mean\\tArousal_Mean\\tValence_SD\\tArousal_SD\\tCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2525\\t人生沒法假設，我沒法假設如果我吐的嚴重心情會怎樣，我只知道如何在自己現有的狀況下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1045\\t「好萊塢報導」指出，今天公佈的倫敦影評人協會獎，臺灣導演李安擊敗同樣入圍奧斯卡金...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138\\t想到故宮的烏鴉都有了歡喜的感覺.\\t6.250\\t4.333\\t0.968\\t0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1781\\t政黨就是理念的結合，華人世界既然只剩下這兩個政黨還相信理智及科學，與我理念相符，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2695\\t107年度綜合所得稅申報31日最後1天，據統計，利用電子申報的民眾已超過6成，到...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No.\\tText\\tValence_Mean\\tArousal_Mean\\tValence_SD\\tArousal_SD\\tCategory\n",
       "0  2525\\t人生沒法假設，我沒法假設如果我吐的嚴重心情會怎樣，我只知道如何在自己現有的狀況下...                     \n",
       "1  1045\\t「好萊塢報導」指出，今天公佈的倫敦影評人協會獎，臺灣導演李安擊敗同樣入圍奧斯卡金...                     \n",
       "2  138\\t想到故宮的烏鴉都有了歡喜的感覺.\\t6.250\\t4.333\\t0.968\\t0....                     \n",
       "3  1781\\t政黨就是理念的結合，華人世界既然只剩下這兩個政黨還相信理智及科學，與我理念相符，...                     \n",
       "4  2695\\t107年度綜合所得稅申報31日最後1天，據統計，利用電子申報的民眾已超過6成，到...                     "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 匹配文件路径（确保在当前工作目录或使用绝对路径）\n",
    "csv_paths = sorted(glob.glob('ChineseEmoBank/CVAT_SD/CVAT_*_SD.csv'))\n",
    "\n",
    "# 逐个读取并合并\n",
    "df_list = [pd.read_csv(path, encoding='utf-8', on_bad_lines='skip') for path in csv_paths]\n",
    "df_cvat_merged = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 输出信息\n",
    "print(\"✅ 合并完成，总样本数：\", len(df_cvat_merged))\n",
    "df_cvat_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b404bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存合并后的 DataFrame 到本地 CSV 文件\n",
    "df_cvat_merged.to_csv('ChineseEmoBank/CVAT_SD/CVAT_all_SD.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9ff825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 数据加载\n",
    "df_phrase = pd.read_csv('ChineseEmoBank/CVAP_SD/CVAP_all_SD.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "df_sentence = pd.read_csv('ChineseEmoBank/CVAS_SD/CVAS_all.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "df_text = pd.read_csv('ChineseEmoBank/CVAT_SD/CVAT_all_SD.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "df_word = pd.read_csv('ChineseEmoBank/CVAW_SD/CVAW_all_SD.csv', sep='\\t')  # 包含 text, valence, arousal 列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "from model import VAModel\n",
    "from dataset import VADataset\n",
    "from utils import compute_metrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df = df.sample(frac=0.8)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "train_set = VADataset(train_df)\n",
    "val_set = VADataset(val_df)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "\n",
    "# 模型初始化\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAModel().to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=len(train_loader) * 5)\n",
    "\n",
    "best_mae = float('inf')\n",
    "patience, patience_counter = 3, 0\n",
    "\n",
    "# 训练\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(**inputs)\n",
    "            preds.append(output)\n",
    "            targets.append(labels)\n",
    "    \n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    metrics = compute_metrics(preds, targets)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val MAE: {metrics['MAE']:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if metrics['MAE'] < best_mae:\n",
    "        best_mae = metrics['MAE']\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
