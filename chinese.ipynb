{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets scikit-learn pandas torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba19d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencc-python-reimplemented\n",
      "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencc-python-reimplemented\n",
      "Successfully installed opencc-python-reimplemented-0.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b4183",
   "metadata": {},
   "source": [
    "✅ 第二步：使用 OpenCC 进行繁简转换\n",
    "\n",
    "✏ 示例：批量转换 CSV 文件中的文本列（如 text 列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051fb1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplified_phrase.csv 保存成功\n",
      "simplified_sentence.csv 保存成功\n",
      "simplified_text.csv 保存成功\n",
      "simplified_word.csv 保存成功\n",
      "繁体转简体完成，保存为 simplified_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from opencc import OpenCC\n",
    "\n",
    "# 初始化转换器：将 繁体 转 简体\n",
    "cc = OpenCC('t2s')  # t2s: Traditional to Simplified\n",
    "\n",
    "# 读取繁体数据\n",
    "df_phrase = pd.read_csv('ChineseEmoBank/CVAP_SD/CVAP_all_SD.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "df_sentence = pd.read_csv('ChineseEmoBank/CVAS_SD/CVAS_all.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "df_text = pd.read_csv('ChineseEmoBank/CVAT_SD/CVAT_all_SD.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "df_word = pd.read_csv('ChineseEmoBank/CVAW_SD/CVAW_all_SD.csv', sep='\\t')  # 包含 text, valence, arousal 列\n",
    "\n",
    "df_phrase.rename(columns={'Phrase': 'text'}, inplace=True)\n",
    "df_word.rename(columns={'Word': 'text'}, inplace=True)\n",
    "df_sentence.rename(columns={'Text': 'text'}, inplace=True)\n",
    "df_text.rename(columns={'Text': 'text'}, inplace=True)\n",
    "\n",
    "# 定义文件保存名\n",
    "output_files = ['simplified_phrase.csv', 'simplified_sentence.csv', 'simplified_text.csv', 'simplified_word.csv']\n",
    "\n",
    "# 批量处理并保存\n",
    "for df, filename in zip([df_phrase, df_sentence, df_text, df_word], output_files):\n",
    "    df = df.copy()  # 复制整个 DataFrame，保留所有列\n",
    "    df['text'] = df['text'].apply(lambda x: cc.convert(str(x)) if pd.notnull(x) else \"\")\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"{filename} 保存成功\")\n",
    "\n",
    "\n",
    "print(\"繁体转简体完成，保存为 simplified_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f81b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取简体数据\n",
    "df_simplified_phrase = pd.read_csv('simplified_phrase.csv')    # 包含 text, valence, arousal\n",
    "df_simplified_sentence = pd.read_csv('simplified_sentence.csv')\n",
    "df_simplified_text = pd.read_csv('simplified_text.csv')\n",
    "df_simplified_word = pd.read_csv('simplified_word.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 设置模型路径（根据所选模型修改）\n",
    "MODEL_NAME = 'hfl/chinese-macbert-base'  # 或者 'hfl/chinese-bert-wwm-ext'\n",
    "\n",
    "# ✅ 模型定义\n",
    "class VARegressionModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.activation = nn.Sigmoid()  # 压缩输出到 [0, 1]\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # [CLS]向量\n",
    "        raw_output = self.regressor(pooled)       # 输出原始值\n",
    "        scaled_output = self.activation(raw_output) * 8 + 1  # 映射到 [1, 9]\n",
    "        return scaled_output\n",
    "\n",
    "# ✅ 自定义数据集\n",
    "class TextVADataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ✅ 加载数据（示例格式）\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)  # 必须有列 ['text', 'valence', 'arousal']\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df[['valence', 'arousal']].values\n",
    "    return texts, labels\n",
    "\n",
    "# ✅ 训练函数\n",
    "def train_model(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ✅ 验证函数\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            preds.append(outputs)\n",
    "            trues.append(labels)\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    rmse = mean_squared_error(trues, preds, squared=False)\n",
    "    return mae, rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 主训练逻辑\n",
    "def main():\n",
    "    # 加载数据\n",
    "    train_texts, train_labels = load_data('train.csv')\n",
    "    val_texts, val_labels = load_data('val.csv')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    train_dataset = TextVADataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextVADataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VARegressionModel(MODEL_NAME).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(f\"\\nEpoch {epoch+1}\")\n",
    "        train_loss = train_model(model, train_loader, optimizer, device)\n",
    "        mae, rmse = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
